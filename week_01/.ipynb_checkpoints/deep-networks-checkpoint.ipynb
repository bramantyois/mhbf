{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"BCCN_logo_berlin.png\" width=\"120\">\n",
    "\n",
    "<h1 id=\"course-title-heading\">\n",
    "    <div style=\"text-align: right\">\n",
    "        Models of Higher Brain Functions\n",
    "        <br>Computer Course\n",
    "        <br>\n",
    "    </div>\n",
    "</h1>\n",
    "    \n",
    "---\n",
    "<div style=\"text-align: left; float: left\">\n",
    "    Lecturer: Prof. Dr. Henning Sprekeler\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: right\">\n",
    "    Assistant: Denis Alevi\n",
    "    <br>(denis.alevi@bccn-berlin.de)\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General exercise instructions\n",
    "The solutions for these exercises (comprising source code, discussion and interpretation in this Jupyter Notebook) should be handed in as a single `.zip` file through the Moodle interface, such that this Jupyter Notebook can be successfully run from the unzipped directory (meaning that e.g. `./helper.py` should be included in the folder).\n",
    "Submission deadline is the start of the next lectue (**10:15 am on Fridays**).\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Some of the exercises will be automatically graded. Make sure that you:\n",
    "1. Fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"\n",
    "2. Remove all `raise NotImplementedError` lines once you inserted your solution\n",
    "3. Don't use variable names starting with underscore (e.g. `_myvar`) in your code, which could interfere with the automatic grading system.\n",
    "\n",
    "Before you start, please fill in below your names and the name of your group as shown on Moodle. Example:\n",
    "```\n",
    "NAMES = [\"Martina Musterfrau\", \"John Smith\"]\n",
    "GROUP = \"A\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = [\"Bramantyo Supriyatno\", \"\"]\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are a few tests to make sure that your installed Python software is not too old\n",
    "import sys\n",
    "assert sys.version_info.major >= 3, \"Your Python version is too old, please update it.\"\n",
    "\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your IPython version is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Exercise-1:-Singular-Value-Mode-Convergence-(10-points)\" data-toc-modified-id=\"Exercise-1:-Singular-Value-Mode-Convergence-(10-points)-1\">Exercise 1: Singular Value Mode Convergence (10 points)</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1:-Implement-the-weight-updates\" data-toc-modified-id=\"1.1:-Implement-the-weight-updates-1.1\">1.1: Implement the weight updates</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-a)-Define-the-covariance-matrices-and-their-SVD-(1-point)\" data-toc-modified-id=\"1.1-a)-Define-the-covariance-matrices-and-their-SVD-(1-point)-1.1.1\">1.1 a) Define the covariance matrices and their SVD (1 point)</a></span></li><li><span><a href=\"#1.1-b)-Implement-weight-updates-for-a-deep-network-(1-point)\" data-toc-modified-id=\"1.1-b)-Implement-weight-updates-for-a-deep-network-(1-point)-1.1.2\">1.1 b) Implement weight updates for a <em>deep</em> network (1 point)</a></span></li><li><span><a href=\"#1.1-c)-Implement-weight-update-for-a-shallow-network-(1-point)\" data-toc-modified-id=\"1.1-c)-Implement-weight-update-for-a-shallow-network-(1-point)-1.1.3\">1.1 c) Implement weight update for a <em>shallow</em> network (1 point)</a></span></li></ul></li><li><span><a href=\"#1.2:-Simulate-learning-and-singular-value-mode-dynamics\" data-toc-modified-id=\"1.2:-Simulate-learning-and-singular-value-mode-dynamics-1.2\">1.2: Simulate learning and singular value mode dynamics</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.2-a)-Integrate-the-weight-dynamics-(2-point)\" data-toc-modified-id=\"1.2-a)-Integrate-the-weight-dynamics-(2-point)-1.2.1\">1.2 a) Integrate the weight dynamics (2 point)</a></span></li><li><span><a href=\"#1.2-b)-Plot-singular-values-modes-(1-point)\" data-toc-modified-id=\"1.2-b)-Plot-singular-values-modes-(1-point)-1.2.2\">1.2 b) Plot singular values modes (1 point)</a></span></li><li><span><a href=\"#1.2-c)-Discuss-your-results-(1-point)\" data-toc-modified-id=\"1.2-c)-Discuss-your-results-(1-point)-1.2.3\">1.2 c) Discuss your results (1 point)</a></span></li></ul></li><li><span><a href=\"#1.3-Compare-simulation-and-theory\" data-toc-modified-id=\"1.3-Compare-simulation-and-theory-1.3\">1.3 Compare simulation and theory</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.3-a)-Implement-the-analytical-solutions-(1-point)\" data-toc-modified-id=\"1.3-a)-Implement-the-analytical-solutions-(1-point)-1.3.1\">1.3 a) Implement the analytical solutions (1 point)</a></span></li><li><span><a href=\"#1.3-b)-Plot-analytical-and-empirical-singulare-value-dynamics-(1-point)\" data-toc-modified-id=\"1.3-b)-Plot-analytical-and-empirical-singulare-value-dynamics-(1-point)-1.3.2\">1.3 b) Plot analytical and empirical singulare value dynamics (1 point)</a></span></li><li><span><a href=\"#1.3-c)-Discuss-your-results-(1-point)\" data-toc-modified-id=\"1.3-c)-Discuss-your-results-(1-point)-1.3.3\">1.3 c) Discuss your results (1 point)</a></span></li></ul></li></ul></li><li><span><a href=\"#Exercise-2:-Deeper-(non-)linear-networks-with-automatic-differentiation-(10-points)\" data-toc-modified-id=\"Exercise-2:-Deeper-(non-)linear-networks-with-automatic-differentiation-(10-points)-2\">Exercise 2: Deeper (non-)linear networks with automatic differentiation (10 points)</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.0-Data-generation-and-PyTorch-examples-(0-points)\" data-toc-modified-id=\"2.0-Data-generation-and-PyTorch-examples-(0-points)-2.1\">2.0 Data generation and PyTorch examples (0 points)</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.0-a)-Generating-data-with-DiffuseTreeSampler\" data-toc-modified-id=\"2.0-a)-Generating-data-with-DiffuseTreeSampler-2.1.1\">2.0 a) Generating data with <code>DiffuseTreeSampler</code></a></span></li><li><span><a href=\"#2.0-b)-PyTorch-example-for-a-feedforward-neural-network\" data-toc-modified-id=\"2.0-b)-PyTorch-example-for-a-feedforward-neural-network-2.1.2\">2.0 b) PyTorch example for a feedforward neural network</a></span></li></ul></li><li><span><a href=\"#2.1:-Implement-a-variable-depth-deep-linear-network-(1-point)\" data-toc-modified-id=\"2.1:-Implement-a-variable-depth-deep-linear-network-(1-point)-2.2\">2.1: Implement a variable depth deep linear network (1 point)</a></span></li><li><span><a href=\"#2.2:-Define-the-online-gradient-descent-training-loop-(3-points)\" data-toc-modified-id=\"2.2:-Define-the-online-gradient-descent-training-loop-(3-points)-2.3\">2.2: Define the online gradient descent training loop (3 points)</a></span></li><li><span><a href=\"#2.3-Generate-a-data-set-and-train-your-deep-linear-network-(3-points):\" data-toc-modified-id=\"2.3-Generate-a-data-set-and-train-your-deep-linear-network-(3-points):-2.4\">2.3 Generate a data set and train your deep linear network (3 points):</a></span></li><li><span><a href=\"#2.4-Train-a-deep-non-linear-network-(3-points)\" data-toc-modified-id=\"2.4-Train-a-deep-non-linear-network-(3-points)-2.5\">2.4 Train a deep non-linear network (3 points)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "611b4f94717347097fff600489bfcf9e",
     "grade": false,
     "grade_id": "cell-c35a5a0c587f76c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Week 1: Learning Dynamics in Deep Linear Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c341d67882ca97f63027c5bf82ce3de9",
     "grade": false,
     "grade_id": "cell-78346dc3a8730613",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports for this exercise, you can't modify these\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import assert function defined in the ./helpers.py file\n",
    "from helpers import assert_var_defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add your additional package imports here\n",
    "\n",
    "# Create plots inline in the Jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "670d821a390cce4b0c4d2fa27d4730dd",
     "grade": false,
     "grade_id": "cell-a32f789a8f18fba8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Singular Value Mode Convergence (10 points)\n",
    "\n",
    "This exercise provides computational insights into the learning dynamics in a *deep* linear network \\& contrasts them with a *shallow* network. The content of this exercise sheet is closely connected to the analytical exercise sheet, where the equations used here are derived. Read throught the analytical exercise sheet once before starting this one (even if you don't intend to hand in the analytical exercise).\n",
    "\n",
    "For the first exercise you are given a set of feature and target covariance matrices:\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\mathbb{E}[xx^T] = \\Sigma^{x} =\n",
    "\t\\begin{bmatrix}\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 & 0\\\\\n",
    "\t0 & 0 & 1 & 0\\\\\n",
    "\t0 & 0 & 0 & 1\\\\\n",
    "    \\end{bmatrix} \\ \\ \\text{   and   } \\ \\\n",
    "\t\\mathbb{E}[yx^T] = \\Sigma^{yx} = \\begin{bmatrix}\n",
    "    1 & 1 & 1 & 1 \\\\\n",
    "    1 & 1 & 0 & 0 \\\\\n",
    "    0 & 0 & 1 & 1 \\\\\n",
    "    1 & 0 & 0 & 0 \\\\\n",
    "    0 & 1 & 0 & 0 \\\\\n",
    "    0 & 0 & 1 & 0 \\\\\n",
    "    0 & 0 & 0 & 1 \\\\\n",
    "    \\end{bmatrix} \\ .\n",
    "\\end{align*}\n",
    "\n",
    "Throughout this exercise the *deep* linear network is defined to have a single hidden layer with 16 hidden units and $W^1 \\in \\mathbb{R}^{16 \\times 4}$ and $W^2 \\in \\mathbb{R}^{7 \\times 16}$:\n",
    "\n",
    "$$\\hat{y} = W^2 W^1 x \\ ,$$\n",
    "\n",
    "while the *shallow* network is a simple input-output mapping without any hidden layers and $W^{\\text{shallow}} \\in \\mathbb{R}^{7 \\times 4}$:\n",
    "\n",
    "$$\\hat{y} = W^{\\text{shallow}}x \\ .$$\n",
    "\n",
    "Furthermore, the singular value decomposition of the input-output covariance matrix can be written as:\n",
    "\n",
    "$$\\Sigma^{yx} = U S V^T \\ ,$$\n",
    "\n",
    "where $S \\in \\mathbb{R}^{4 \\times 4}$ denotes the singular value diagonal matrix with non-zero elements $s_\\alpha, \\alpha=1, \\dots, 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0c459369ac183fc2a4ebdac329ad829",
     "grade": false,
     "grade_id": "instructions_1-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1: Implement the weight updates\n",
    "\n",
    "First, you will implement the mean weight update equations in the continuous time limit using forward Euler integration.\n",
    "\n",
    "The mean weight update equations for the *deep* network are:\n",
    "\n",
    "\\begin{align}\n",
    "\\tau \\frac{d}{dt} W^1 &= W^{2^T} (\\Sigma^{yx} - W^2 W^1 \\Sigma^x) \\\\\\\\\n",
    "\\tau \\frac{d}{dt} W^2 &= (\\Sigma^{yx} - W^2 W^1 \\Sigma^x)W^{1^T}\n",
    "\\end{align}\n",
    "\n",
    "And for the *shallow* network:\n",
    "\n",
    "\\begin{align}\n",
    "\\tau \\frac{d}{dt} W^{\\text{shallow}} &= \\Sigma^{yx} - W^{\\text{shallow}} \\Sigma^x\n",
    "\\end{align}\n",
    "\n",
    "To understand where the update equations come from, have a look at the analytical exercise sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e3ba731f57557a83d7a4247f51eed7f",
     "grade": false,
     "grade_id": "instructions_1-1-a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.1 a) Define the covariance matrices and their SVD (1 point)\n",
    "Define two variables `sigma_x` and `sigma_yx` for the covariance matrices $\\Sigma^x$ and $\\Sigma^{yx}$. Then compute the singular value decomposition (SVD) of the input-output covariance matrix (e.g. by using [`np.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)). Store the resulting singular value diagonal matrix $S$ in a variable called `s` (lower case).\n",
    "\n",
    "This task will be automatically graded. Please make sure that you use the variable names as instructed here and that you delete the `raise NotImplementedError()` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64e43377e95dd7757a114816dcef8322",
     "grade": false,
     "grade_id": "cell-d49058ec16985ebf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-15b94d1fa268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9829d9047e4617497632e759542e3b0",
     "grade": false,
     "grade_id": "cell-4f4adcd4f290d20c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following code cell will check if you defined the correct variable names. Run it to make sure you did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5255003a6e04dde3c06b130e5998cf78",
     "grade": true,
     "grade_id": "cell-e35d86e0a4a6d6d6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Test that variables `sigma_x`, `sigma_yx` and `s` are defined \"\"\"\n",
    "\n",
    "for varname in [\"sigma_x\", \"sigma_yx\", \"s\"]:\n",
    "    assert_var_defined(varname)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4382a9ad3c14b7f0b90613fd763bee74",
     "grade": false,
     "grade_id": "cell-e21e2cbd0f6e65d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.1 b) Implement weight updates for a *deep* network (1 point)\n",
    "Implement the weight updates for the *deep* linear network in the function `update_deep_network_weights` below (don't change the order of function arguments and return values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be229d4d303c9cd207707ae96b33ca08",
     "grade": false,
     "grade_id": "update_linear_network",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_deep_network_weights(w1, w2, dt, tau, sigma_x, sigma_yx):\n",
    "    \"\"\"\n",
    "    Update the deep network weights for one integration time step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w1 : numpy.ndarray\n",
    "        W1 weight matrix, connecting input to hidden layer\n",
    "    w2 : numpy.ndarray\n",
    "        W2 weight matrix, connection hidden to output layer\n",
    "    dt : float\n",
    "        Integration time step for forward Euler\n",
    "    tau : float\n",
    "        Time constant of the weight changes (inverse of learning rate)\n",
    "    sigma_x : numpy.ndarray\n",
    "        Input covariance matrix\n",
    "    sigma_yx : numpy.ndarray\n",
    "        Input-output covariance matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w1_updated : numpy.ndarray\n",
    "        Updated weight matrix W1\n",
    "    w2_updated : numpy.ndarray\n",
    "        Updated weight matrix W2\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return w1_updated, w2_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f9bbce08405027b1f2469619297f838",
     "grade": true,
     "grade_id": "cell-1fa960487c3d2159",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Test that the function is defined as instructed \"\"\"\n",
    "\n",
    "assert_var_defined(\"update_deep_network_weights\", func=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "100a4b2b93ca5435098503511c310f78",
     "grade": false,
     "grade_id": "cell-1cf561e74e4e5b8b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.1 c) Implement weight update for a *shallow* network (1 point)\n",
    "Implement the weights updates for the *shallow* linear network in the function `update_deep_shallow_weights` below (don't change the the order of function arguments and return values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8cf83b8058ada04a8a4a51da517e79c",
     "grade": false,
     "grade_id": "update_shallow_network",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_shallow_network_weights(w, dt, tau, sgima_x, sigma_yx):\n",
    "    \"\"\"\n",
    "    Update the shallow network weights for one integration time step.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : numpy.ndarray\n",
    "        W_shallow weight matrix, connecting input to hidden layer\n",
    "    dt : float\n",
    "        Integration time step for forward Euler\n",
    "    tau : float\n",
    "        Time constant of the weight changes (inverse of learning rate)\n",
    "    sigma_x : numpy.ndarray\n",
    "        Input covariance matrix\n",
    "    sigma_yx : numpy.ndarray\n",
    "        Input-output covariance matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w_updated : numpy.ndarray\n",
    "        Updated weight matrix W_shallow\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return w_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34f0a89f43c8bc62d3ad2d19e24fe74e",
     "grade": true,
     "grade_id": "cell-f9710997e41dd938",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Test that the function is defined as instructed \"\"\"\n",
    "\n",
    "assert_var_defined(\"update_shallow_network_weights\", func=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f88218be6bbea969b58e18a19199ebb8",
     "grade": false,
     "grade_id": "instructions_1-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2: Simulate learning and singular value mode dynamics\n",
    "\n",
    "For the special case of no input correlations ($\\Sigma^X$ is the unit matrix in our example), we get the following fixed point conditions (marked by $*$) from the weight update equations (to see this, set $\\frac{d}{dt} = 0$):\n",
    "\n",
    "- For the *deep* network:\n",
    "\n",
    "$$\n",
    "W^{2,*}W^{1,*} = \\Sigma^{yx} \\\\\n",
    "$$\n",
    "    \n",
    "- For the *shallow* network:\n",
    "\n",
    "$$\n",
    "W^{\\text{shallow},*} = \\Sigma^{yx} \\\\\n",
    "$$\n",
    "    \n",
    "That means the linear network's input-output mapping learns the input-output covariance structure. To investigate the singular value mode dynamics, we can compute the SVDs on the covariance matrix $\\Sigma^{\\hat{y}x}(t)$ between the inputs $x$ and the predicted outputs $\\hat{y}$ at a given time $t$ during learning:\n",
    "\n",
    "- For the *deep* network, this corresponds to the SVD of the matrix product $W^2(t)W^1(t)$:\n",
    "\n",
    "$$\n",
    "\\Sigma^{\\hat{y}x} = W^2(t)W^1(t) = U A(t) V^T \\\\\n",
    "$$\n",
    "    \n",
    "- For the *shallow* network, this corresponds to the SVD of the matrix $W^{\\text{shallow}}$:\n",
    "\n",
    "$$\n",
    "\\Sigma^{\\hat{y}x} = W^{\\text{shallow}}(t) = U B(t) V^T \\\\\n",
    "$$\n",
    "\n",
    "This gives us the singular value modes $a_\\alpha(t)$ and $b_\\alpha(t)$, which are the diagonal elements of the diagonal matrices $A(t)$ and $B(t)$ ($\\alpha = 1, ... 4$).\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15c1071b3dd3ce0416178d7e33765f6d",
     "grade": false,
     "grade_id": "instructions_1-2-a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.2 a) Integrate the weight dynamics (2 point)\n",
    "Perform weight updates in the *deep* and *shallow* networks for the given covariance matrices $\\Sigma^{x}$ and $\\Sigma^{yx}$ (use the functions defined in 1.1). Start with Gaussian initialised weight matrices (with mean $\\mu = 0$ and standard deviation $\\sigma = 0.01$). Set the time constant $\\tau = 1/\\eta$ with learning rate $\\eta=0.4$ and integrate for a total time $T = 15$. After each update of the weight matrices, compute the singular value modes $a_\\alpha(t)$ and $b_\\alpha(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a3455dfc8239da9a60a6ed59f4cc985",
     "grade": true,
     "grade_id": "cell-624c126f6b919f68",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dc558dac359e451fab1d3ae6901febd",
     "grade": false,
     "grade_id": "cell-ba89c77174e63c22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.2 b) Plot singular value modes (1 point)\n",
    "Plot the dynamics of the singular value modes $a_\\alpha(t)$ and $b_\\alpha(t)$, together with the singular values $s_{\\alpha}$ of the covariance matrix $\\Sigma^{yx}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b045edc5c8cace986d7dce988762704d",
     "grade": true,
     "grade_id": "cell-3ad4b0f63957a675",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "216c0911ab78ed9888eeb10dfa316da7",
     "grade": false,
     "grade_id": "cell-03418ea394e8a1a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.2 c) Discuss your results (1 point)\n",
    "How do the singular value modes between the *deep* and *shallow* network compare to each other and to the singular values of the input-output covariance matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "871c090c7737d031d58a9630cb44ad0c",
     "grade": true,
     "grade_id": "cell-85ad80140d94f58b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d589d0c09adbc2735959c8b2f051d783",
     "grade": false,
     "grade_id": "instructions_1-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Compare simulation and theory\n",
    "For the linear neural networks which you implemented here, the singular value mode dynamics can be computed analytically. The analytical solutions are:\n",
    "\n",
    "- For the *deep* network:\n",
    "\n",
    "$$\n",
    "a_\\alpha(t) = \\frac{s_\\alpha e^{2s_\\alpha t/\\tau}}{e^{2s_\\alpha t/\\tau} - 1 + s_\\alpha/a^0_\\alpha}\n",
    "$$\n",
    "\n",
    "- For the *shallow* network:\n",
    "\n",
    "$$\n",
    "b_\\alpha(t) = s_\\alpha(1-e^{-t/\\tau}) + b_\\alpha^0e^{-t/\\tau}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fffb0dd156344e22bd2acfb08e97b7e5",
     "grade": false,
     "grade_id": "instructions_1-3-a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.3 a) Implement the analytical solutions (1 point)\n",
    "Compute the singular value mode dynamics (for different time steps between $t = 0$  and $t = T$) using the analytic solutions with $a_\\alpha^0 = b_\\alpha^0 = 0.001$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f71c70cdbec87d7550b9fba4edba77c7",
     "grade": true,
     "grade_id": "cell-11833a995822dc15",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50918bbe35e0c041a227dc88f357219c",
     "grade": false,
     "grade_id": "instructions_1-3-b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.3 b) Plot analytical and empirical singulare value dynamics (1 point)\n",
    "Plot the analytically calculated singular value mode dynamics together with the empirical ones simulated in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b1afe5668c395bf981bb5da2f4bde95",
     "grade": true,
     "grade_id": "cell-28c643984d5e4caa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a347dfa405973d365979dacba6a8c7c",
     "grade": false,
     "grade_id": "instructions_1-3-c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 1.3 c) Discuss your results (1 point)\n",
    "How well do theory and simulation align?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbea00eeb158a043045f35d0e2d027c0",
     "grade": true,
     "grade_id": "cell-8396cc513cda391f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7d02d4a8e2fc5fc0493e9bad19b83c3",
     "grade": false,
     "grade_id": "instructions-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: Deeper (non-)linear networks with automatic differentiation (10 points)\n",
    "\n",
    "In this exercise we will probe whether the theoretical results \\& insights from the previous exercise translate to deeper linear networks as well as non-linear networks.\n",
    "\n",
    "Deep Learning is powered by reverse mode automatic differentiation, computational graphs \\& stochastic gradient descent algorithms.\n",
    "We will use the Python library PyTorch for automatic differenatiation due to its easy installation, documentation \\& recent popularity in academic research. If you are unfamiliar with PyTorch, you might find [this introductions](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) useful. Don't worry, the trained networks are fairly small, do not involve convolutions \\& can therefore easily be trained on any CPU.\n",
    "\n",
    "You can install PyTorch e.g. via `pip install pytorch` or `conda install pytorch`. If it doesn't work out of the box or if you want to have GPU support, you can find a suitable installation command at https://pytorch.org/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab91f9ba411f2135bd2b978b39eeb796",
     "grade": false,
     "grade_id": "cell-711863766a1ce74f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports for this exercise, you can't modify these\n",
    "from collections import OrderedDict\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ModuleNotFoundError:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"You need pytorch for this exercise. See installation instructions above.\"\n",
    "    )\n",
    "\n",
    "import torch.nn as nn\n",
    "    \n",
    "from helpers import DiffuseTreeSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9426dad7ef735e827c3f4bc3b064479",
     "grade": false,
     "grade_id": "instructions-2-0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.0 Data generation and PyTorch examples (0 points)\n",
    "\n",
    "This section will give an example for how to create a neural network with PyTorch and how to generate data to train it in the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c9e2a648fdeb9eef93ba5f4dd883acb",
     "grade": false,
     "grade_id": "instructions-2-0-a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.0 a) Generating data with `DiffuseTreeSampler`\n",
    "\n",
    "In order to train a neural network on the same kind of task as was done in exercise 1, we need a data set $\\{\\bf{x}^\\alpha, y^\\alpha\\}_{\\alpha = 1, \\cdots, N}$ with $N$ data samples, each consisting of an item vector $\\bf{x}^\\alpha$ (the input to our network) and its associated property or feature vector $\\bf{y}^\\alpha$ (the output of our network). We will use the `DiffuseTreeSampler` class to generate such data throughout this exercise. It is defined in `./helpers.py` and implements a hierarchical data-generation process (**no need to implement anything from scratch, you can just used it as shown below**).\n",
    "\n",
    "The hierarchically structured data is generated through a branching diffusion\n",
    "process (see image below). To generate a single feature $y_i^\\alpha$ for multiple items $\\alpha$, an initial binary value is determined through a coin flip at the top of the hierarchy. The sign of this value flips with a small probability along each link in the tree. At the bottom, this yields the value of one feature $y_i^\\alpha$ across items $\\alpha$. Many features are generated by repeatedly sampling from this process independently. The $\\pm 1$ values depicted in the figure below are one possible sampling.\n",
    "\n",
    "<img src=\"diffusion-tree-sampler.png\" width=\"400\">\n",
    "\n",
    "This figure comes from the Supplementary Information in [Saxe et al. (2019)](https://www.pnas.org/content/116/23/11537), where you can find more information on this process.\n",
    "\n",
    "Below is an example on how to generate data with the `DiffuseTreeSampler` class and how to compute the singular values for the generated data set. The hierarchical structue in the data can be visualized by plotting the covariance matrix of the features.\n",
    "\n",
    "Vary the parameters of the `DiffuseTreeSampler` and see how that changes the structre in the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data samples: 64\n",
      "Feature vector dimension: 100\n",
      "Item vector dimension: 64\n",
      "\n",
      "First feature vector:\n",
      "[ 1  1 -1  1  1  1 -1 -1  1  1  1 -1  1  1  1 -1  1  1 -1  1 -1 -1  1 -1\n",
      " -1 -1 -1  1 -1 -1  1 -1  1  1  1  1  1  1 -1 -1  1  1  1 -1 -1  1 -1 -1\n",
      " -1 -1  1  1 -1  1  1  1 -1 -1  1  1 -1 -1  1 -1  1 -1 -1  1 -1 -1 -1 -1\n",
      "  1  1  1  1  1 -1  1  1 -1  1  1 -1  1  1  1  1  1 -1 -1  1  1  1  1  1\n",
      " -1 -1 -1  1]\n",
      "\n",
      "First item vector:\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the dataset by instantiating and sampling\n",
    "hierarchical_tree = DiffuseTreeSampler(feature_dim=100, tree_depth=3,\n",
    "                                       branching_factor=4, sample_epsilon=0.05)\n",
    "features, items = hierarchical_tree.sample_data()\n",
    "\n",
    "assert features.shape[0] == items.shape[0]\n",
    "print(f\"Number of data samples: {features.shape[0]}\")\n",
    "print(f\"Feature vector dimension: {features.shape[1]}\")\n",
    "print(f\"Item vector dimension: {items.shape[1]}\\n\")\n",
    "print(f\"First feature vector:\\n{features[0]}\\n\")\n",
    "print(f\"First item vector:\\n{items[0]}\")\n",
    "plt.matshow(np.cov(features))\n",
    "plt.title(\"Item covariance matrix\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the singular values of the generated dataset\n",
    "sigma_yx = features.T @ items\n",
    "U, s, V = np.linalg.svd(sigma_yx, full_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00ed5c43a8d4aa8ded94063d360c8e93",
     "grade": false,
     "grade_id": "cell-3d8807025aa85b3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### 2.0 b) PyTorch example for a feedforward neural network\n",
    "\n",
    "Here, you can see an example of a feedforward neural network defined in PyTorch and how it can be trained on the data we generated in the previous section. The important steps shown below are:\n",
    "\n",
    "1. Define a neural network architecture (here a single hidden layer with ReLU activation function). In PyTorch, this is done by inheriting from the [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class and implementing the `forward` method, which should generate the network output for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afcda5975d43e317f725108384b1925d",
     "grade": false,
     "grade_id": "cell-8bc195cc22795f86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ExampleNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a deep neural network in PyTorch. In this case with a single hidden layer\n",
    "    with ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        This is the class constructor. It is called when you intantiate an instance of\n",
    "        this class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : float\n",
    "            Dimension of the network input\n",
    "        output_dim : float\n",
    "            Dimension of the network output\n",
    "        \n",
    "        Example\n",
    "        -------\n",
    "        >>> network_instance = ExampleNetwork()\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the constructor (`__init__`) of the parent class (nn.Module)\n",
    "        super().__init__()\n",
    "\n",
    "        # Define a dictionary that collects the different layers.\n",
    "        # Afterwards, this dictionary provides the input to the nn.Sequential model\n",
    "        layers = OrderedDict()\n",
    "        layers[\"input_to_hidden\"] = nn.Linear(input_dim, 64, bias=False)\n",
    "        layers[\"input_to_hidden_activation\"] = nn.ReLU()\n",
    "        layers[\"hidden_to_output\"] = nn.Linear(64, output_dim, bias=False)\n",
    "        self.model = nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        This method implements the forward path of the neural network. Every PyTorch\n",
    "        network that inherits from `nn.Module` has to define this function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor : torch.tensor\n",
    "            The input to the network\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            The output of the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Propagate the input through the linear network\n",
    "        return self.model(input_tensor.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1510431e83ba861f2eeaf10610674de2",
     "grade": false,
     "grade_id": "cell-304d3a49a540eb1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "2. Create a network instance, an optimizer object (here we use stochastic gradient descent ([`torch.optim.SGD`](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD))) and a loss function (here we use mean squared error loss ([`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a574976cd1434f3dbfc4523704f82d3",
     "grade": false,
     "grade_id": "cell-9913a70682f53871",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Network Architecture:\n",
      "\n",
      "ExampleNetwork(\n",
      "  (model): Sequential(\n",
      "    (input_to_hidden): Linear(in_features=64, out_features=64, bias=False)\n",
      "    (input_to_hidden_activation): ReLU()\n",
      "    (hidden_to_output): Linear(in_features=64, out_features=100, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_dim = items.shape[1]\n",
    "output_dim = features.shape[1]\n",
    "\n",
    "learning_rate = 0.5\n",
    "network_instance = ExampleNetwork(input_dim, output_dim)\n",
    "sgd_optimizer = torch.optim.SGD(network_instance.parameters(), lr=learning_rate)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "print(\"The Network Architecture:\\n\")\n",
    "print(network_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c499fe5e0dff7a17fce00b8f1f7bd3af",
     "grade": false,
     "grade_id": "cell-26eaca3f8838d0fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "3. Perform a forward pass of the network to get predictions and calculate the loss. To perform the forward pass, the network instance has to be called directly with `network_instance()`. This will call the `nn.Module.__call__` method, which will use your `ExampleNetwork.forward` method to calculate the forward pass and will add some PyTorch magic that will compute the gradients for you.\n",
    "4. Reset any gradients that might have been stored before to $0$ and perform a backward pass to calculate the new gradients based on your last forward pass. Finally, perform one SGD optimization step to update your network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da1bced62e53f6758c74bd3f90749f81",
     "grade": false,
     "grade_id": "cell-1c0080ecff5b4873",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE Loss for the 1st data point is 1.001\n",
      "Gradients input to hidden weights after reset: None\n",
      "Gradient sum input to hidden weights after backward pass: -0.006666247732937336\n"
     ]
    }
   ],
   "source": [
    "### 3.\n",
    "# Get the first feature and target from your dataset\n",
    "input_tensor = torch.tensor(items[0])\n",
    "y_true = torch.tensor(features[0]).float()\n",
    "\n",
    "# Perform a forward pass through the network\n",
    "y_hat = network_instance(input_tensor)\n",
    "\n",
    "# Calculate the loss for the first data point\n",
    "loss = mse_loss(y_hat, y_true)\n",
    "print(\"The MSE Loss for the 1st data point is {:.3f}\".format(loss)) \n",
    "\n",
    "### 4.\n",
    "# Reset gradients\n",
    "network_instance.zero_grad()\n",
    "gradient_sum_before = network_instance.model.input_to_hidden.weight.grad\n",
    "print(f\"Gradients input to hidden weights after reset: {gradient_sum_before}\")\n",
    "\n",
    "# Compute new gradients by performing a backword pass\n",
    "loss.backward()\n",
    "gradient_sum_after = network_instance.model.input_to_hidden.weight.grad.sum()\n",
    "print(f\"Gradient sum input to hidden weights after backward pass: {gradient_sum_after}\")\n",
    "\n",
    "# Update the weights using the SGD optimizer\n",
    "sgd_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4873c66baa89cd97806809cd80b73a99",
     "grade": false,
     "grade_id": "cell-2dae8cdffddd8564",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.1: Implement a variable depth deep linear network (1 point)\n",
    "Code a variable depth deep linear network class that takes as an input a list of hidden units for each layer (e.g. `hidden_units=[16, 32, 32, 16]`). Use the `DeepLinearNetwork` class skeleton defined below. In the `### YOUR CODE HERE` section, loop over the hidden layers in the network to define the full network architecture (use `nn.Linear` as already done for the first and last layers of the network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d0f16e906844780f6cdfec806efa1c7",
     "grade": false,
     "grade_id": "cell-e9132021e073cae2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DeepLinearNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A deep liner neural network with a variable number of hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=64, output_dim=100, hidden_units=[64]):\n",
    "        \"\"\"\n",
    "        This is the class constructor. It is called when you intantiate an instance of\n",
    "        this class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : float\n",
    "            Dimension of the network input\n",
    "        output_dim : float\n",
    "            Dimension of the network output\n",
    "        \"\"\"\n",
    "            \n",
    "        # Call the class constructor (__init__) of the parent class (nn.Module)\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define a dictionary that collects the different layers\n",
    "        # This dictionary will provide the input to the nn.Sequential model\n",
    "        layers = OrderedDict()\n",
    "        layers[\"input_to_hidden\"] = nn.Linear(\n",
    "            input_dim, hidden_units[0], bias=False\n",
    "        )           \n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #layers['hidden_0_activation'] = nn.ReLU()\n",
    "        \n",
    "        #for i in range(len(hidden_units)-1):\n",
    "        #    name = 'hidden_' + str(i+1)\n",
    "        #    name_activation = 'hidden_' + str(i+1) + '_activation' \n",
    "        #    layers[name] = nn.Linear(hidden_units[i], hidden_units[i+1], bias=False)\n",
    "        \n",
    "        #    layers[name_activation] = nn.ReLU()  \n",
    "        \n",
    "        layers[\"hidden_to_output\"] = nn.Linear(\n",
    "            #hidden_units[-1], \n",
    "            hidden_units[0]\n",
    "            output_dim, bias=False\n",
    "        )\n",
    "        self.model = nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        This method implements the forward path of the neural network. Every PyTorch\n",
    "        network that inherits from `nn.Module` has to define this function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor : torch.tensor\n",
    "            The input to the network\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            The output of the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Propagate the input through the linear network\n",
    "        return self.model(input_tensor.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0844fa487f5ddd6b3da8557b7c2c432d",
     "grade": true,
     "grade_id": "cell-050ae1339be2e5ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Test that the class is defined as instructed \"\"\"\n",
    "\n",
    "assert_var_defined(\"DeepLinearNetwork\", func=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a5c5491e524dd80d82b9c94b2224be4",
     "grade": false,
     "grade_id": "cell-8f10cb1c4916e6b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2: Define the online gradient descent training loop (3 points)\n",
    "Complement the learning loop defined in the `train_network` function below. This will require multiple small steps:\n",
    "\n",
    "1. Shuffle the data ordering at the beginning of each training epoch.\n",
    "2. Perform the forward pass at each iteration for a selected input-target pair.\n",
    "3. Calculate the corresponding loss using the mean squared error (MSE) loss.\n",
    "4. Reset the parameter gradients, perform a backwards pass to calculate the current gradients \\& update the parameters with the help of an optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d068cc55a66fd5c8d46bc61e982ee9b2",
     "grade": false,
     "grade_id": "cell-ab40a468bf4b638a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_network(network, loss_func, optimizer, num_epochs, inputs, targets):\n",
    "    \"\"\"\n",
    "    Train a neural network by running the training loop\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : nn.Module\n",
    "        An instance of a PyTorch neural network class, that inherits from `nn.Module`,\n",
    "        e.g. an instance of the `DeepLinearNetwork` class.\n",
    "    loss_func : nn._Loss\n",
    "        A PyTorch loss criterion, e.g. an instance of `nn.MLELoss`, which measures the\n",
    "        mean squared error (MSE) between each element of two vectors.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        PyTorch optimizer object, e.g. an instance of `torch.optim.SGE`, which implements\n",
    "        stochastic gradien descent (SGD)\n",
    "    num_epochs : int\n",
    "        Number of training loops over the entire dataset\n",
    "    inputs : numpy.ndarray\n",
    "        Inputs used for training the network\n",
    "    targets : numpy.ndarray\n",
    "        Output targets for computing the training loss during training\n",
    "    \"\"\"\n",
    "    loss_log = []\n",
    "    log_singular_vals = []\n",
    "    num_points = targets.shape[0]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        ### 1. Shuffle the data ordering\n",
    "        # YOUR CODE HERE\n",
    "        idx_shuffled = np.arange(0, inputs.shape[0])\n",
    "        np.random.shuffle(idx_shuffled)\n",
    "        \n",
    "        inputs = inputs[idx_shuffled,:]\n",
    "        targets = targets[idx_shuffled,:]\n",
    "                \n",
    "        # Loop over all examples in an Online SGD Loop\n",
    "        for t in range(num_points):\n",
    "            # Extract the current training datapoint and transform it into a Torch Tensor\n",
    "            input_tensor = torch.tensor(inputs[t])\n",
    "            y_true = torch.tensor(targets[t]).float()\n",
    "                \n",
    "            ### 2. Perform the forward pass, which computes the prediction for the single\n",
    "            ###    datapoint y^hat\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            y_hat = network.forward(input_tensor)\n",
    "            \n",
    "            ### 3. Compute the corresponding loss\n",
    "            # YOUR CODE HERE\n",
    "            loss = mse_loss(y_hat, y_true)\n",
    "                     \n",
    "            # 4. Clear the gradients, Perform the backward pass, and SGD update\n",
    "            # YOUR CODE HERE\n",
    "            network.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the epoch loss tracker\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log the mean epoch loss & calculate the SVD\n",
    "        loss_log.append(epoch_loss/num_points)\n",
    "        # Note that `inputs` is a unit matrix\n",
    "        y_hat_full = network(torch.tensor(inputs)).detach().numpy()\n",
    "        U, s, V = np.linalg.svd(y_hat_full.T, full_matrices=True)\n",
    "        log_singular_vals.append(s)\n",
    "\n",
    "    return loss_log, np.array(log_singular_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abd11abd1bb60cc911f0829f447bfa29",
     "grade": true,
     "grade_id": "cell-b7d568000b82a79a",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Test that the function is defined as instructed \"\"\"\n",
    "\n",
    "assert_var_defined(\"train_network\", func=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc08adc0ab1a3ba9269edf6d12969355",
     "grade": false,
     "grade_id": "cell-9a9e5fc4af6816d1",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### 2.3 Generate a data set and train your deep linear network (3 points):\n",
    "Create a dataset by creating an instance of the `DiffuseTreeSampler` class and sampling from it. Afterwards, compute the SVD of $\\Sigma^{yx}$ and assert that $\\Sigma^x = \\mathbf{1}$. Then, train a deep linear network:\n",
    "\n",
    "1. Instantiate your `DeepLinearNetwork` with three hidden layers (`hidden_units=[64, 128, 128]`).\n",
    "2. Instantiate a stochastic gradient descent optimizer object with learning rate $\\eta = 0.5$.\n",
    "3. Define the mean squared error (MSE) loss function.\n",
    "4. Run the `train_network` loop for 200 epochs.\n",
    "\n",
    "After each epoch (loop over the entire dataset) compute the SVD of the covariance matrix $\\Sigma^{\\hat{y}x}$, which is (as in exercise 1) the product of the linear network weight matrices (since we have no input correlations). Note that in this case the product of weight matrices can be simply obtained by forward propagating a unit matrix through the network. Plot the evolution of the singular value modes over the course of the learning epochs as well as the static singular values of $\\Sigma^{yx}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b5edfa02358c44603ef0903cf8e07af",
     "grade": true,
     "grade_id": "cell-aa2b73577e75eacd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "hierarchical_tree = DiffuseTreeSampler(feature_dim=100, tree_depth=3,\n",
    "                                       branching_factor=4, sample_epsilon=0.05)\n",
    "features, items = hierarchical_tree.sample_data()\n",
    "\n",
    "sigma_yx = features.T @ items\n",
    "U, s, V = np.linalg.svd(sigma_yx, full_matrices=True)\n",
    "\n",
    "hidden_units = [64, 128, 128]\n",
    "\n",
    "input_dim = items.shape[1]\n",
    "output_dim = features.shape[1]\n",
    "\n",
    "learning_rate = 0.5\n",
    "network = DeepLinearNetwork(input_dim, output_dim, hidden_units)\n",
    "sgd_optimizer = torch.optim.SGD(network_instance.parameters(), lr=learning_rate)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "num_epoch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfaa42c268908d4e53dad44ea2ee3704",
     "grade": false,
     "grade_id": "cell-fb494155de2fe7f4",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### 2.4 Train a deep non-linear network (3 points)\n",
    "Do the results hold up for both deep linear and non-linear networks? Implement a ReLU activation after every linear layer and repeat the steps in exercise 2.3. Which singular values converge first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b964a9549da443df579876857f3c12e1",
     "grade": true,
     "grade_id": "cell-46c47df521076fc9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282,\n",
       "  1.0002283286303282],\n",
       " array([[3.9303023e-01, 3.6462739e-01, 3.3306965e-01, ..., 2.6528258e-03,\n",
       "         2.1729076e-03, 2.3421523e-04],\n",
       "        [3.9303023e-01, 3.6462739e-01, 3.3306965e-01, ..., 2.6528258e-03,\n",
       "         2.1729076e-03, 2.3421523e-04],\n",
       "        [3.9303023e-01, 3.6462739e-01, 3.3306965e-01, ..., 2.6528258e-03,\n",
       "         2.1729076e-03, 2.3421523e-04],\n",
       "        ...,\n",
       "        [3.9303023e-01, 3.6462739e-01, 3.3306965e-01, ..., 2.6528258e-03,\n",
       "         2.1729076e-03, 2.3421523e-04],\n",
       "        [3.9303023e-01, 3.6462739e-01, 3.3306965e-01, ..., 2.6528258e-03,\n",
       "         2.1729076e-03, 2.3421523e-04],\n",
       "        [3.9303023e-01, 3.6462739e-01, 3.3306965e-01, ..., 2.6528258e-03,\n",
       "         2.1729076e-03, 2.3421523e-04]], dtype=float32))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "train_network(network, mse_loss, sgd_optimizer, num_epoch, items, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.54px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
